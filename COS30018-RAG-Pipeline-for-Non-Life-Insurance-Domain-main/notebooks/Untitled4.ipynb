{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686,
          "referenced_widgets": [
            "09d0aeb7693541dcb2835fe2d02fc162",
            "1d80ed4601274606809a64cc2956268d",
            "73d5e50a718046b8b3a46bed0c82d9b4",
            "72a0a31e9bae42a19c8ede27f518703c",
            "82ccd22856ad4a5e89c3c78fbed7fc7c",
            "720aa60dc07743119f633a9d0a46f9e1",
            "076bee5627a941f49a03dba9088a9757",
            "4485506e0da14e509eabfc2fa5acdfe3",
            "b66b63b7dfc547a6bf63e65f6aaa1c2f",
            "7c29722e50454fb8ba86ed22532ba746",
            "c2057573cd434edf99a46412f27bae20"
          ]
        },
        "id": "5BD8IzfiDoUn",
        "outputId": "7d445a6c-0032-4eef-94c9-789d7fecc532"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'frontend'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPyPDF2\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfplumber\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\cos30018\\.venv\\Lib\\site-packages\\fitz\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfrontend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtools\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mop\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'frontend'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm.notebook import tqdm\n",
        "# Add at the top with other imports\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from dotenv import load_dotenv\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Define the paths to the financial reports\n",
        "pdf_paths = [\n",
        "    r\"tt-files/BVH_Baocaotaichinh_Q3_2024_Hopnhat.pdf\",\n",
        "    r\"tt-files/BVH_Baocaotaichinh_Q4_2024_Hopnhat.pdf\"\n",
        "]\n",
        "\n",
        "class FinancialReportProcessor:\n",
        "    def __init__(self, pdf_paths):\n",
        "        self.pdf_paths = pdf_paths\n",
        "        self.output_dir = os.path.join(os.path.dirname(pdf_paths[0]), \"processed_chunks\")\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def extract_text_with_pdfplumber(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF using pdfplumber to better maintain layout.\"\"\"\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    page_text = page.extract_text(x_tolerance=3, y_tolerance=3)\n",
        "                    if page_text:\n",
        "                        text += f\"\\n\\n--- Page {page_num + 1} ---\\n\\n\" + page_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {pdf_path} with pdfplumber: {e}\")\n",
        "            # Fallback to PyPDF2\n",
        "            text = self.extract_text_with_pypdf2(pdf_path)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def extract_text_with_pypdf2(self, pdf_path: str) -> str:\n",
        "        \"\"\"Fallback extractor using PyPDF2.\"\"\"\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                num_pages = len(reader.pages)\n",
        "\n",
        "                for page_num in range(num_pages):\n",
        "                    page = reader.pages[page_num]\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += f\"\\n\\n--- Page {page_num + 1} ---\\n\\n\" + page_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {pdf_path} with PyPDF2: {e}\")\n",
        "\n",
        "        return text\n",
        "\n",
        "    def clean_financial_text(self, text: str) -> str:\n",
        "        \"\"\"Clean OCR errors common in financial documents.\"\"\"\n",
        "        # Replace common OCR errors in financial reports\n",
        "        replacements = {\n",
        "            r'\\(VND\\)': ' VND',\n",
        "            r'\\b,\\b': '.',  # Replace isolated commas with decimal points\n",
        "            r'[\\u2002\\u2003\\u2004\\u2005\\u2006\\u2007\\u2008\\u2009\\u200a]+': ' ',  # Replace various space chars\n",
        "            r'[^\\S\\r\\n]+': ' ',  # Multiple spaces to single space\n",
        "            r'\\n{3,}': '\\n\\n',  # Multiple newlines to double newline\n",
        "        }\n",
        "\n",
        "        for pattern, replacement in replacements.items():\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def detect_document_structure(self, text: str) -> Dict[str, List[Tuple[str, int, int]]]:\n",
        "        \"\"\"\n",
        "        Detect the document structure by identifying section headers and their positions.\n",
        "        Returns a dictionary of sections with their start and end positions.\n",
        "        \"\"\"\n",
        "        # Common section headers in financial reports\n",
        "        section_patterns = [\n",
        "            r\"(THÔNG TIN CHUNG)\",\n",
        "            r\"(BẢNG CÂN ĐỐI KẾ TOÁN HỢP NHẤT[^\\n]*)\",\n",
        "            r\"(BÁO CÁO KẾT QUẢ HOẠT ĐỘNG KINH DOANH HỢP NHẤT[^\\n]*)\",\n",
        "            r\"(BÁO CÁO LƯU CHUYỂN TIỀN TỆ HỢP NHẤT[^\\n]*)\",\n",
        "            r\"(THUYẾT MINH BÁO CÁO TÀI CHÍNH HỢP NHẤT[^\\n]*)\",\n",
        "            r\"(MỤC LỤC)\",\n",
        "            r\"(\\d+\\.\\s+[A-Z\\s][A-ZĐÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ\\s]+)\"\n",
        "        ]\n",
        "\n",
        "        sections = {}\n",
        "        for pattern in section_patterns:\n",
        "            for match in re.finditer(pattern, text, re.MULTILINE):\n",
        "                section_title = match.group(1).strip()\n",
        "                sections[section_title] = (section_title, match.start(), match.end())\n",
        "\n",
        "        # Sort sections by position\n",
        "        sorted_sections = sorted(sections.values(), key=lambda x: x[1])\n",
        "\n",
        "        # Determine section bounds (start of one section to start of next)\n",
        "        structured_sections = []\n",
        "        for i in range(len(sorted_sections)):\n",
        "            section_title, start_pos, _ = sorted_sections[i]\n",
        "            end_pos = sorted_sections[i+1][1] if i < len(sorted_sections) - 1 else len(text)\n",
        "            structured_sections.append((section_title, start_pos, end_pos))\n",
        "\n",
        "        return structured_sections\n",
        "\n",
        "    def extract_metadata(self, text: str, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract metadata from the document text.\"\"\"\n",
        "        metadata = {\n",
        "            \"source\": os.path.basename(file_path),\n",
        "            \"file_path\": file_path\n",
        "        }\n",
        "\n",
        "        # Extract quarter and year\n",
        "        quarter_match = re.search(r'Q(\\d+)_(\\d{4})', file_path)\n",
        "        if quarter_match:\n",
        "            metadata[\"quarter\"] = quarter_match.group(1)\n",
        "            metadata[\"year\"] = quarter_match.group(2)\n",
        "\n",
        "        # Extract company info\n",
        "        if re.search(r'Tập đoàn Bảo Việt', text, re.IGNORECASE):\n",
        "            metadata[\"company\"] = \"Tập đoàn Bảo Việt\"\n",
        "            metadata[\"ticker\"] = \"BVH\"\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def classify_section_type(self, section_title: str, section_text: str) -> str:\n",
        "        \"\"\"Classify the type of financial report section.\"\"\"\n",
        "        if re.search(r'bảng cân đối', section_title, re.IGNORECASE):\n",
        "            return \"balance_sheet\"\n",
        "        elif re.search(r'kết quả.*kinh doanh', section_title, re.IGNORECASE):\n",
        "            return \"income_statement\"\n",
        "        elif re.search(r'lưu chuyển tiền tệ', section_title, re.IGNORECASE):\n",
        "            return \"cash_flow\"\n",
        "        elif re.search(r'thuyết minh', section_title, re.IGNORECASE):\n",
        "            return \"notes\"\n",
        "        elif re.search(r'thông tin chung', section_title, re.IGNORECASE):\n",
        "            return \"general_info\"\n",
        "        elif re.search(r'^\\d+\\.', section_title.strip()):\n",
        "            return \"note_item\"\n",
        "        else:\n",
        "            return \"other\"\n",
        "\n",
        "    def is_tabular_section(self, section_text: str) -> bool:\n",
        "        \"\"\"Determine if a section contains financial tables.\"\"\"\n",
        "        # Look for patterns indicating tables (lots of numbers, aligned data)\n",
        "        if re.search(r'(\\d{1,3}(\\.|\\,)\\d{3}){2,}', section_text):\n",
        "            return True\n",
        "        # Count number of lines with aligned data patterns\n",
        "        lines = section_text.split('\\n')\n",
        "        aligned_lines = 0\n",
        "        for line in lines:\n",
        "            if re.search(r'\\d+\\s+[\\d\\.,]+\\s+[\\d\\.,]+', line):\n",
        "                aligned_lines += 1\n",
        "        return aligned_lines > 5\n",
        "\n",
        "    def chunk_financial_report(self, text: str, file_path: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Split a financial report into semantic chunks based on document structure,\n",
        "        optimized for RAG with special handling for tabular and text content.\n",
        "        Returns a list of dictionaries with text content and metadata.\n",
        "        \"\"\"\n",
        "        chunks = []\n",
        "        base_metadata = self.extract_metadata(text, file_path)\n",
        "\n",
        "        # Clean text\n",
        "        text = self.clean_financial_text(text)\n",
        "\n",
        "        # Detect sections\n",
        "        structured_sections = self.detect_document_structure(text)\n",
        "\n",
        "        # Process each section\n",
        "        for section_title, start_pos, end_pos in structured_sections:\n",
        "            section_text = text[start_pos:end_pos].strip()\n",
        "            section_type = self.classify_section_type(section_title, section_text)\n",
        "\n",
        "            if section_type in [\"balance_sheet\", \"income_statement\", \"cash_flow\"]:\n",
        "                # Financial statements: keep tables intact\n",
        "                # Split into logical sub-tables if needed\n",
        "                # For balance sheet: assets, liabilities, equity\n",
        "                if self.is_tabular_section(section_text):\n",
        "                    # Special handling for tables - prevent chunking within tables\n",
        "                    tables = self.extract_logical_tables(section_text)\n",
        "                    for i, table_text in enumerate(tables):\n",
        "                        metadata = base_metadata.copy()\n",
        "                        metadata.update({\n",
        "                            \"section\": section_title,\n",
        "                            \"section_type\": section_type,\n",
        "                            \"chunk_type\": \"financial_statement_table\",\n",
        "                            \"table_index\": i\n",
        "                        })\n",
        "                        chunks.append({\n",
        "                            \"content\": table_text.strip(),\n",
        "                            \"metadata\": metadata\n",
        "                        })\n",
        "                else:\n",
        "                    # If no tables detected, keep as is\n",
        "                    metadata = base_metadata.copy()\n",
        "                    metadata.update({\n",
        "                        \"section\": section_title,\n",
        "                        \"section_type\": section_type,\n",
        "                        \"chunk_type\": \"financial_statement\"\n",
        "                    })\n",
        "                    chunks.append({\n",
        "                        \"content\": section_text.strip(),\n",
        "                        \"metadata\": metadata\n",
        "                    })\n",
        "\n",
        "            elif section_type == \"notes\" or section_type == \"note_item\":\n",
        "                # Notes: chunk by note number\n",
        "                note_chunks = self.chunk_notes_section(section_text, section_title)\n",
        "                for note_title, note_content in note_chunks:\n",
        "                    metadata = base_metadata.copy()\n",
        "                    metadata.update({\n",
        "                        \"section\": section_title if section_type == \"notes\" else \"THUYẾT MINH BÁO CÁO TÀI CHÍNH HỢP NHẤT\",\n",
        "                        \"note\": note_title,\n",
        "                        \"section_type\": section_type,\n",
        "                        \"chunk_type\": \"financial_note\"\n",
        "                    })\n",
        "                    chunks.append({\n",
        "                        \"content\": f\"{note_title}\\n\\n{note_content}\".strip(),\n",
        "                        \"metadata\": metadata\n",
        "                    })\n",
        "\n",
        "            else:\n",
        "                # General information and other sections: semantic chunking\n",
        "                semantic_chunks = self.chunk_by_semantic_paragraphs(section_text, section_title)\n",
        "                for chunk_title, chunk_content in semantic_chunks:\n",
        "                    metadata = base_metadata.copy()\n",
        "                    metadata.update({\n",
        "                        \"section\": section_title,\n",
        "                        \"section_type\": section_type,\n",
        "                        \"chunk_type\": \"general_info\",\n",
        "                        \"subsection\": chunk_title if chunk_title != section_title else \"\"\n",
        "                    })\n",
        "                    chunks.append({\n",
        "                        \"content\": chunk_content.strip(),\n",
        "                        \"metadata\": metadata\n",
        "                    })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def extract_logical_tables(self, section_text: str) -> List[str]:\n",
        "        \"\"\"Split financial tables into logical subcomponents.\"\"\"\n",
        "        # Look for common dividing patterns in tables\n",
        "        table_divisions = [\n",
        "            r\"TÀI SẢN\",\n",
        "            r\"NGUỒN VỐN\",\n",
        "            r\"CHỈ TIÊU\",\n",
        "            r\"Lưu chuyển tiền từ hoạt động\",\n",
        "        ]\n",
        "\n",
        "        # Find table divisions\n",
        "        divisions = [(0, \"Start\")]\n",
        "        for pattern in table_divisions:\n",
        "            for match in re.finditer(pattern, section_text):\n",
        "                divisions.append((match.start(), pattern))\n",
        "        divisions.append((len(section_text), \"End\"))\n",
        "\n",
        "        # Sort divisions by position\n",
        "        divisions.sort()\n",
        "\n",
        "        # Extract tables between divisions\n",
        "        tables = []\n",
        "        for i in range(len(divisions) - 1):\n",
        "            start_pos = divisions[i][0]\n",
        "            end_pos = divisions[i+1][0]\n",
        "            table_text = section_text[start_pos:end_pos].strip()\n",
        "            if table_text and len(table_text.split()) > 10:  # Skip very small chunks\n",
        "                tables.append(table_text)\n",
        "\n",
        "        # If no divisions found, return the entire section\n",
        "        if len(tables) == 0:\n",
        "            return [section_text]\n",
        "\n",
        "        return tables\n",
        "\n",
        "    def chunk_notes_section(self, section_text: str, section_title: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Chunk notes section into individual notes.\n",
        "        Returns a list of (note_title, note_content) tuples.\n",
        "        \"\"\"\n",
        "        # Pattern to identify note headers\n",
        "        note_pattern = r\"(\\d+\\.?\\s+[A-ZĐÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ\\s]+)\"\n",
        "\n",
        "        # Split by note headers\n",
        "        note_sections = re.split(note_pattern, section_text)\n",
        "        chunks = []\n",
        "\n",
        "        # If section_title is a note title itself\n",
        "        if re.search(r\"^\\d+\\.\", section_title.strip()):\n",
        "            chunks.append((section_title, section_text))\n",
        "            return chunks\n",
        "\n",
        "        # Otherwise, process all notes in the section\n",
        "        if len(note_sections) > 1:\n",
        "            for i in range(1, len(note_sections), 2):\n",
        "                if i+1 < len(note_sections):\n",
        "                    note_title = note_sections[i].strip()\n",
        "                    note_content = note_sections[i+1].strip()\n",
        "\n",
        "                    # Handle long notes by splitting them into context-preserving chunks\n",
        "                    if len(note_content.split()) > 800:\n",
        "                        # Find logical divisions within the note\n",
        "                        subsections = self.find_note_subsections(note_content)\n",
        "                        if subsections:\n",
        "                            for subsection in subsections:\n",
        "                                chunks.append((note_title, subsection))\n",
        "                        else:\n",
        "                            # If no subsections, chunk by paragraphs with overlap\n",
        "                            chunks.append((note_title, note_content))\n",
        "                    else:\n",
        "                        chunks.append((note_title, note_content))\n",
        "        else:\n",
        "            # If no notes found, return the whole section\n",
        "            chunks.append((section_title, section_text))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def find_note_subsections(self, note_content: str) -> List[str]:\n",
        "        \"\"\"Find logical subsections within a note.\"\"\"\n",
        "        # Look for subsection patterns like a), b), c) or i), ii), iii)\n",
        "        subsection_pattern = r'(?:^|\\n)(?:[a-z]\\)|\\([a-z]\\)|\\(?\\d+\\)|\\([ivx]+\\))'\n",
        "\n",
        "        # Find subsection starts\n",
        "        subsection_matches = list(re.finditer(subsection_pattern, note_content, re.MULTILINE))\n",
        "\n",
        "        if len(subsection_matches) <= 1:\n",
        "            return []\n",
        "\n",
        "        # Extract subsections\n",
        "        subsections = []\n",
        "        for i in range(len(subsection_matches)):\n",
        "            start_pos = subsection_matches[i].start()\n",
        "            end_pos = subsection_matches[i+1].start() if i < len(subsection_matches) - 1 else len(note_content)\n",
        "            subsection_text = note_content[start_pos:end_pos].strip()\n",
        "            if subsection_text and len(subsection_text.split()) > 15:  # Skip very small chunks\n",
        "                subsections.append(subsection_text)\n",
        "\n",
        "        return subsections\n",
        "\n",
        "    def chunk_by_semantic_paragraphs(self, section_text: str, section_title: str) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Chunk text by semantic paragraphs, respecting context.\n",
        "        Returns a list of (subsection_title, subsection_content) tuples.\n",
        "        \"\"\"\n",
        "        # Look for subsection headers (all caps or with specific formats)\n",
        "        subsection_pattern = r'(?:^|\\n)([A-ZĐÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ][A-ZĐÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ\\s]{2,}(?:\\n|\\:))'\n",
        "\n",
        "        # Find subsection headers\n",
        "        subsection_matches = list(re.finditer(subsection_pattern, section_text))\n",
        "\n",
        "        if not subsection_matches:\n",
        "            # No subsections found, check if text is long enough to split\n",
        "            if len(section_text.split()) > 800:\n",
        "                # Split by paragraphs with contextual overlap\n",
        "                paragraphs = re.split(r'\\n\\s*\\n', section_text)\n",
        "                chunks = []\n",
        "                current_chunk = \"\"\n",
        "\n",
        "                for para in paragraphs:\n",
        "                    if not para.strip():\n",
        "                        continue\n",
        "\n",
        "                    if len(current_chunk.split()) + len(para.split()) > 800:\n",
        "                        chunks.append((section_title, current_chunk.strip()))\n",
        "                        # Start new chunk with 20% overlap for context\n",
        "                        current_chunk = para\n",
        "                    else:\n",
        "                        if current_chunk:\n",
        "                            current_chunk += \"\\n\\n\"\n",
        "                        current_chunk += para\n",
        "\n",
        "                # Add final chunk\n",
        "                if current_chunk:\n",
        "                    chunks.append((section_title, current_chunk.strip()))\n",
        "\n",
        "                return chunks\n",
        "            else:\n",
        "                # Text is short enough to keep as a single chunk\n",
        "                return [(section_title, section_text)]\n",
        "\n",
        "        # Extract subsections\n",
        "        chunks = []\n",
        "        for i in range(len(subsection_matches)):\n",
        "            start_pos = subsection_matches[i].start()\n",
        "            end_pos = subsection_matches[i+1].start() if i < len(subsection_matches) - 1 else len(section_text)\n",
        "            subsection_title = subsection_matches[i].group(1).strip()\n",
        "            subsection_text = section_text[start_pos:end_pos].strip()\n",
        "\n",
        "            if subsection_text and len(subsection_text.split()) > 15:  # Skip very small chunks\n",
        "                chunks.append((subsection_title, subsection_text))\n",
        "\n",
        "        # Include text before the first subsection\n",
        "        if subsection_matches and subsection_matches[0].start() > 0:\n",
        "            prefix_text = section_text[:subsection_matches[0].start()].strip()\n",
        "            if prefix_text and len(prefix_text.split()) > 15:\n",
        "                chunks.insert(0, (section_title, prefix_text))\n",
        "\n",
        "        # If no valid chunks, return the whole section\n",
        "        if not chunks:\n",
        "            return [(section_title, section_text)]\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_all_reports(self):\n",
        "        \"\"\"Process all financial reports and save the chunks.\"\"\"\n",
        "        all_chunks = []\n",
        "\n",
        "        for pdf_path in tqdm(self.pdf_paths, desc=\"Processing PDF files\"):\n",
        "            print(f\"\\nProcessing {os.path.basename(pdf_path)}...\")\n",
        "\n",
        "            # Extract text from PDF\n",
        "            doc_text = self.extract_text_with_pdfplumber(pdf_path)\n",
        "\n",
        "            # If extraction failed, skip this file\n",
        "            if not doc_text:\n",
        "                print(f\"Failed to extract text from {pdf_path}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Chunk the document\n",
        "            doc_chunks = self.chunk_financial_report(doc_text, pdf_path)\n",
        "            print(f\"Generated {len(doc_chunks)} chunks from this document\")\n",
        "\n",
        "            # Add numeric IDs to chunks for this document\n",
        "            for i, chunk in enumerate(doc_chunks):\n",
        "                chunk[\"metadata\"][\"chunk_id\"] = f\"{os.path.basename(pdf_path)}_chunk_{i}\"\n",
        "\n",
        "            all_chunks.extend(doc_chunks)\n",
        "            print(f\"Completed processing {os.path.basename(pdf_path)}\")\n",
        "\n",
        "        # Save chunks\n",
        "        self.save_chunks_for_rag(all_chunks)\n",
        "\n",
        "        # Print statistics\n",
        "        self.print_chunk_statistics(all_chunks)\n",
        "\n",
        "        # Return chunks for further processing\n",
        "        return all_chunks\n",
        "\n",
        "    def save_chunks_for_rag(self, chunks: List[Dict[str, Any]]):\n",
        "        \"\"\"Save chunks to files for use in a RAG system.\"\"\"\n",
        "        # Save the chunks to a CSV file\n",
        "        df = pd.DataFrame([{\n",
        "            \"chunk_id\": chunk[\"metadata\"][\"chunk_id\"],\n",
        "            \"content\": chunk[\"content\"],\n",
        "            \"source\": chunk[\"metadata\"][\"source\"],\n",
        "            \"section\": chunk[\"metadata\"].get(\"section\", \"\"),\n",
        "            \"note\": chunk[\"metadata\"].get(\"note\", \"\"),\n",
        "            \"section_type\": chunk[\"metadata\"].get(\"section_type\", \"\"),\n",
        "            \"chunk_type\": chunk[\"metadata\"].get(\"chunk_type\", \"\"),\n",
        "            \"quarter\": chunk[\"metadata\"].get(\"quarter\", \"\"),\n",
        "            \"year\": chunk[\"metadata\"].get(\"year\", \"\"),\n",
        "        } for chunk in chunks])\n",
        "\n",
        "        csv_path = os.path.join(self.output_dir, \"financial_report_chunks.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Saved {len(chunks)} chunks to {csv_path}\")\n",
        "\n",
        "        # Save sample chunks for inspection\n",
        "        sample_dir = os.path.join(self.output_dir, \"sample_chunks\")\n",
        "        os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "        for i, chunk in enumerate(chunks[:10]):  # Save first 10 chunks as samples\n",
        "            sample_path = os.path.join(sample_dir, f\"chunk_{i}.txt\")\n",
        "            with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"Metadata: {chunk['metadata']}\\n\\n\")\n",
        "                f.write(chunk[\"content\"])\n",
        "\n",
        "    def print_chunk_statistics(self, chunks: List[Dict[str, Any]]):\n",
        "        \"\"\"Print statistics about the chunks.\"\"\"\n",
        "        print(f\"\\nTotal number of chunks: {len(chunks)}\")\n",
        "\n",
        "        df_stats = pd.DataFrame([{\n",
        "            \"file\": chunk[\"metadata\"][\"source\"],\n",
        "            \"chunk_type\": chunk[\"metadata\"].get(\"chunk_type\", \"unknown\"),\n",
        "            \"section_type\": chunk[\"metadata\"].get(\"section_type\", \"unknown\"),\n",
        "            \"word_count\": len(chunk[\"content\"].split())\n",
        "        } for chunk in chunks])\n",
        "\n",
        "        print(\"\\nChunking statistics:\")\n",
        "        print(df_stats.groupby([\"file\", \"chunk_type\"]).agg({\n",
        "            \"word_count\": [\"count\", \"mean\", \"min\", \"max\"]\n",
        "        }).round(1))\n",
        "\n",
        "        print(\"\\nSample chunks:\")\n",
        "        for i, chunk in enumerate(chunks[:3]):\n",
        "            print(f\"\\nChunk {i+1}:\")\n",
        "            print(f\"Metadata: {chunk['metadata']}\")\n",
        "            print(f\"Content preview: {chunk['content'][:200]}...\")\n",
        "            print(f\"Word count: {len(chunk['content'].split())}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    # Add these methods to the FinancialReportProcessor class in Untitled4.ipynb\n",
        "\n",
        "def get_vietnamese_embeddings(self, texts: list) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embeddings using the same Vietnamese model as in data_ingestion.ipynb\n",
        "    \"\"\"\n",
        "    # Use the same model as in the data_ingestion.ipynb file\n",
        "    model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
        "    \n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "def upload_to_qdrant(self, chunks: List[Dict[str, Any]], embeddings: np.ndarray, batch_size=32):\n",
        "    \"\"\"\n",
        "    Upload chunks and their embeddings to Qdrant vector database.\n",
        "    \"\"\"\n",
        "    # Load environment variables\n",
        "    load_dotenv('.env.local')  # Adjust path if needed\n",
        "    \n",
        "    # Qdrant configuration\n",
        "    QDRANT_URL = os.getenv('QDRANT_URL')\n",
        "    QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
        "    COLLECTION_NAME = 'legal_docs'  # Using the same collection as in data_ingestion.ipynb\n",
        "    \n",
        "    print(f\"Connecting to Qdrant at {QDRANT_URL}\")\n",
        "    \n",
        "    # Initialize Qdrant client\n",
        "    client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=QDRANT_API_KEY,\n",
        "        prefer_grpc=False,\n",
        "        timeout=60\n",
        "    )\n",
        "    \n",
        "    # Test connection\n",
        "    try:\n",
        "        collections = client.get_collections()\n",
        "        print(f\"Successfully connected to Qdrant\")\n",
        "    except Exception as e:\n",
        "        print(f\"Connection failed: {str(e)}\")\n",
        "        print(\"Please check your Qdrant URL and API key\")\n",
        "        return False\n",
        "    \n",
        "    # Create collection if it doesn't exist\n",
        "    embedding_size = embeddings.shape[1]\n",
        "    if not client.collection_exists(COLLECTION_NAME):\n",
        "        print(f\"Creating collection '{COLLECTION_NAME}'...\")\n",
        "        client.create_collection(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            vectors_config=VectorParams(\n",
        "                size=embedding_size,\n",
        "                distance=Distance.COSINE\n",
        "            )\n",
        "        )\n",
        "        print(f\"Collection created successfully\")\n",
        "    else:\n",
        "        print(f\"Collection '{COLLECTION_NAME}' already exists\")\n",
        "    \n",
        "    # Upload data in batches\n",
        "    total_batches = (len(chunks) + batch_size - 1) // batch_size\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for batch_idx in range(total_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(chunks))\n",
        "        \n",
        "        # Prepare batch of points\n",
        "        points = []\n",
        "        for i in range(start_idx, end_idx):\n",
        "            # Create a unique ID for each chunk\n",
        "            chunk_id = i + 1  # Start IDs from 1, matching data_ingestion.ipynb approach\n",
        "            \n",
        "            # Prepare payload with exact metadata field names from data_ingestion.ipynb\n",
        "            payload = {\n",
        "                \"text\": chunks[i][\"content\"],\n",
        "                \"metadata\": {\n",
        "                    \"source\": chunks[i][\"metadata\"][\"source\"],\n",
        "                    \"section\": chunks[i][\"metadata\"].get(\"section\", \"\"),\n",
        "                    \"section_type\": chunks[i][\"metadata\"].get(\"section_type\", \"\"),\n",
        "                    \"chunk_type\": chunks[i][\"metadata\"].get(\"chunk_type\", \"\"),\n",
        "                    \"chunk_id\": chunks[i][\"metadata\"][\"chunk_id\"]\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            # Add financial-specific metadata fields\n",
        "            if \"quarter\" in chunks[i][\"metadata\"]:\n",
        "                payload[\"metadata\"][\"quarter\"] = chunks[i][\"metadata\"][\"quarter\"]\n",
        "            if \"year\" in chunks[i][\"metadata\"]:\n",
        "                payload[\"metadata\"][\"year\"] = chunks[i][\"metadata\"][\"year\"]\n",
        "            if \"company\" in chunks[i][\"metadata\"]:\n",
        "                payload[\"metadata\"][\"company\"] = chunks[i][\"metadata\"][\"company\"]\n",
        "            if \"ticker\" in chunks[i][\"metadata\"]:\n",
        "                payload[\"metadata\"][\"ticker\"] = chunks[i][\"metadata\"][\"ticker\"]\n",
        "            if \"note\" in chunks[i][\"metadata\"]:\n",
        "                payload[\"metadata\"][\"note\"] = chunks[i][\"metadata\"][\"note\"]\n",
        "            \n",
        "            points.append({\n",
        "                \"id\": chunk_id,\n",
        "                \"vector\": embeddings[i].tolist(),\n",
        "                \"payload\": payload\n",
        "            })\n",
        "        \n",
        "        # Upload batch\n",
        "        try:\n",
        "            client.upsert(\n",
        "                collection_name=COLLECTION_NAME,\n",
        "                points=points\n",
        "            )\n",
        "            print(f\"Batch {batch_idx + 1}/{total_batches} uploaded ({start_idx + 1}-{end_idx} of {len(chunks)})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to upload batch {batch_idx + 1}: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    # Print upload statistics\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nUpload completed successfully!\")\n",
        "    print(f\"Total documents: {len(chunks)}\")\n",
        "    print(f\"Total time: {total_time:.2f} seconds\")\n",
        "    \n",
        "    return True\n",
        "\n",
        "def process_and_upload_to_qdrant(self):\n",
        "    \"\"\"\n",
        "    Process all financial reports, generate embeddings, and upload to Qdrant.\n",
        "    \"\"\"\n",
        "    print(\"Starting processing and uploading to Qdrant...\")\n",
        "    \n",
        "    # Process all reports to get chunks\n",
        "    all_chunks = self.process_all_reports()\n",
        "    \n",
        "    if not all_chunks or len(all_chunks) == 0:\n",
        "        print(\"No chunks were generated. Check your PDF paths.\")\n",
        "        return\n",
        "    \n",
        "    # Extract text from chunks for embedding\n",
        "    texts = [chunk[\"content\"] for chunk in all_chunks]\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(texts)} chunks...\")\n",
        "    embeddings = self.get_vietnamese_embeddings(texts)\n",
        "    print(f\"Embeddings generated with shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Upload chunks and embeddings to Qdrant\n",
        "    print(\"Uploading to Qdrant database...\")\n",
        "    success = self.upload_to_qdrant(all_chunks, embeddings)\n",
        "    \n",
        "    if success:\n",
        "        print(\"Financial reports successfully uploaded to Qdrant\")\n",
        "    else:\n",
        "        print(\"Failed to upload financial reports to Qdrant\")\n",
        "\n",
        "\n",
        "# At the main execution part of the notebook, add:\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting financial report processing...\")\n",
        "\n",
        "    # Create processor with all PDF paths\n",
        "    processor = FinancialReportProcessor(pdf_paths)\n",
        "    \n",
        "    # Process all reports, generate embeddings, and upload to Qdrant\n",
        "    processor.get_vietnamese_embeddings = get_vietnamese_embeddings\n",
        "    processor.upload_to_qdrant = upload_to_qdrant\n",
        "    processor.process_and_upload_to_qdrant = process_and_upload_to_qdrant\n",
        "    \n",
        "    # Run the upload process\n",
        "    processor.process_and_upload_to_qdrant()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "076bee5627a941f49a03dba9088a9757": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09d0aeb7693541dcb2835fe2d02fc162": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d80ed4601274606809a64cc2956268d",
              "IPY_MODEL_73d5e50a718046b8b3a46bed0c82d9b4",
              "IPY_MODEL_72a0a31e9bae42a19c8ede27f518703c"
            ],
            "layout": "IPY_MODEL_82ccd22856ad4a5e89c3c78fbed7fc7c"
          }
        },
        "1d80ed4601274606809a64cc2956268d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_720aa60dc07743119f633a9d0a46f9e1",
            "placeholder": "​",
            "style": "IPY_MODEL_076bee5627a941f49a03dba9088a9757",
            "value": "Processing PDF files: 100%"
          }
        },
        "4485506e0da14e509eabfc2fa5acdfe3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "720aa60dc07743119f633a9d0a46f9e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a0a31e9bae42a19c8ede27f518703c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c29722e50454fb8ba86ed22532ba746",
            "placeholder": "​",
            "style": "IPY_MODEL_c2057573cd434edf99a46412f27bae20",
            "value": " 2/2 [00:00&lt;00:00, 134.54it/s]"
          }
        },
        "73d5e50a718046b8b3a46bed0c82d9b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4485506e0da14e509eabfc2fa5acdfe3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b66b63b7dfc547a6bf63e65f6aaa1c2f",
            "value": 2
          }
        },
        "7c29722e50454fb8ba86ed22532ba746": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ccd22856ad4a5e89c3c78fbed7fc7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b66b63b7dfc547a6bf63e65f6aaa1c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2057573cd434edf99a46412f27bae20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
